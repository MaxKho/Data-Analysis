{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataAnalysis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Installing necessary software\n",
        "!pip install nltk\n",
        "!pip install spacy==3.3.0\n",
        "!pip install spacytextblob\n",
        "!python -m textblob.download_corpora\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "8NoOxmfKqKXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import random\n",
        "from random import choice\n",
        "import queue\n",
        "import threading\n",
        "import time\n",
        "from numpy.core.fromnumeric import mean\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import spacy\n",
        "from spacytextblob.spacytextblob import SpacyTextBlob"
      ],
      "metadata": {
        "id": "Ga_gXUWIBfEe"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Given dataset too small; Downloading a bigger one\n",
        "nltk.download(\"twitter_samples\")\n",
        "\n",
        "# Downloading stopwords to filter later\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QX4iCW2aqSqs",
        "outputId": "2af6ea9c-9fa3-4c05-d4eb-9ea9d40c7d8a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling downloaded Twitter data into a dataframe\n",
        "pos = nltk.corpus.twitter_samples.strings(\"positive_tweets.json\")\n",
        "neg = nltk.corpus.twitter_samples.strings(\"negative_tweets.json\")\n",
        "\n",
        "twitter_extra = pd.concat([pd.DataFrame(list(zip(pos, \n",
        "                                                 ['P' for i in range(len(pos))],\n",
        "                                                 ['Twitter' for i in range(len(pos))])),\n",
        "                                        columns = [0, 0, 0]),\n",
        "                          pd.DataFrame(list(zip(neg, \n",
        "                                                ['N' for i in range(len(neg))],\n",
        "                                                ['Twitter' for i in range(len(neg))])),\n",
        "                                       columns = [0, 0, 0])])"
      ],
      "metadata": {
        "id": "J4wjgdaDq6VS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qancWjI_A7cp"
      },
      "outputs": [],
      "source": [
        "# Compiling the rest of the data into dataframes\n",
        "fbd = pd.read_csv(\"fb_data.txt\", sep = \"\\n\", header = None)\n",
        "fbl = pd.read_csv(\"fb_label.txt\", sep = \"\\n\", header = None)\n",
        "fbs = pd.DataFrame(['Facebook' for i in range(len(fbd))])\n",
        "amd = pd.read_csv(\"amazon_data.txt\", sep = \"\\n\", header = None)\n",
        "aml = pd.read_csv(\"amazon_label.txt\", sep = \"\\n\", header = None)\n",
        "ams = pd.DataFrame(['Amazon' for i in range(len(amd))])\n",
        "twd = pd.read_csv(\"tweets_data.txt\", sep = \"\\n\", header = None)\n",
        "twl = pd.read_csv(\"tweets_label.txt\", sep = \"\\n\", header = None)\n",
        "tws = pd.DataFrame(['Twitter' for i in range(len(twd))])\n",
        "\n",
        "facebook = pd.concat([fbd, fbl, fbs], axis = 1)\n",
        "amazon = pd.concat([amd, aml, ams], axis = 1)\n",
        "twitter_base = pd.concat([twd, twl, tws], axis = 1)\n",
        "twitter = pd.concat([twitter_base, twitter_extra])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenating all of the data into a single dataset\n",
        "df = pd.concat([facebook, amazon, twitter])\n",
        "x = df.iloc[:, [0, 2]].values\n",
        "y = df.iloc[:, 1].values\n",
        "\n",
        "# Splitting the dataset into train data, on which we will train our models, and\n",
        "# test data, which will be used to simulate social media users\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, \n",
        "                                                    random_state = 0)\n",
        "# Constructing a dataset to later sample simulated comments and replies from; \n",
        "# sentiment classification is removed for realism\n",
        "simul = pd.concat([pd.DataFrame(x_test).iloc[:, 0], \n",
        "                   pd.DataFrame(['U' for i in range(len(x_test))]),\n",
        "                   pd.DataFrame(x_test).iloc[:, 1]],\n",
        "                  axis = 1)"
      ],
      "metadata": {
        "id": "CitRfKcJCZR7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the publication class, to which both comments and replies belong\n",
        "class Publication:\n",
        "  def __init__(self, content, sentiment, source, date, time):\n",
        "      self.content = content\n",
        "      self.sentiment = sentiment\n",
        "      self.source = source\n",
        "      self.date = date\n",
        "      self.time = time\n",
        "\n",
        "# Defining the user class, which is designed to simulate real internet users\n",
        "class User:\n",
        "  def __init__(self, comments, replies, userID, name):\n",
        "    self.comments = comments\n",
        "    self.replies = replies\n",
        "    self.userID = userID\n",
        "    self.name = name\n",
        "\n",
        "# Defining the room class\n",
        "class Room:\n",
        "  def __init__(self, members, join_requests, name):\n",
        "    self.members = members\n",
        "    self.join_requests = join_requests\n",
        "    self.name = name"
      ],
      "metadata": {
        "id": "pdEdKBkXif4K"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialising rooms and users\n",
        "rooms = [Room([], [], 'Room%s' % (i+1)) for i in range(2)]\n",
        "# Only 2 users because the computational strain is otherwise too high\n",
        "users = [User([], [], [], []) for i in range(2)]\n",
        "\n",
        "for user in users:\n",
        "  comments = []\n",
        "  replies = []\n",
        "  # Creating sample names for users\n",
        "  names = [\"Jack\", \"John\", \"Max\", \"Arseniy\", \"Gregorz\", \"Mike\", \n",
        "           \"GoogleEnPassant\", \"GaryChess\", \"Jacob\", \"George\",\n",
        "           \"HolyHell\", \"Rhosnes\", \"Mitch\", \"Moh\"]\n",
        "  numbers = list(range(1000))\n",
        "  dates = [1, 2, 3, 4, 5]\n",
        "  for i in range(int(np.random.normal(10, 5))):\n",
        "    # Randomly sampling comments and replies from the dataset and assigning them\n",
        "    # to our users\n",
        "    comment = simul.sample()\n",
        "    reply = simul.sample()\n",
        "    comments.append(Publication(comment.iloc[0, 0], \n",
        "                                comment.iloc[0, 1],\n",
        "                                comment.iloc[0, 2], \n",
        "                                choice(dates),\n",
        "                                0))\n",
        "    replies.append(Publication(reply.iloc[0, 0], \n",
        "                               reply.iloc[0, 1],\n",
        "                               reply.iloc[0, 2],\n",
        "                               choice(dates),\n",
        "                               0))\n",
        "  user.comments = comments\n",
        "  user.replies = replies\n",
        "  user.userID = users.index(user)\n",
        "  # Randomly assigning names to users\n",
        "  user.name = '%s%d' % (choice(names), choice(numbers))\n",
        "  # Randomly selecting whether the users are going to be members or will request\n",
        "  # access to each of the rooms\n",
        "  for i in range(len(rooms)):\n",
        "    status = choice([1, 2, 3, 4])\n",
        "    if status == 1:\n",
        "      rooms[i].members.append(user)\n",
        "    if status == 2:\n",
        "      rooms[i].join_requests.append(user)"
      ],
      "metadata": {
        "id": "YHBSpV16m68o"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialising sentiment analysis components\n",
        "tokeniser = RegexpTokenizer(pattern = '\\w+|[:()@]+') # Emotics are vital to sentiment\n",
        "# analysis and have raised the accuracy score of each of the models to be tested\n",
        "# at least 5-fold\n",
        "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
        "ps = PorterStemmer()"
      ],
      "metadata": {
        "id": "BMDRONy4STUh"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing a clean piece of text ready for vectorisation\n",
        "def clean(text, get_tokens = False):\n",
        "  text = text.lower()\n",
        "  tokens = tokeniser.tokenize(text)\n",
        "  # Removing stopwords and hyperlinks, which aren't useful for sentiment analysis\n",
        "  clean_tokens = [token for token in tokens if token not in stopwords\n",
        "                  and not any(x in token for x in ['www', '.com', 'http'])]\n",
        "  stemmed_tokens = [ps.stem(token) for token in clean_tokens]\n",
        "  if not get_tokens:\n",
        "    return \" \".join(stemmed_tokens)\n",
        "  # Creating a second purpose for the function to break down a piece of text into\n",
        "  # a list of clean words and symbols\n",
        "  else:\n",
        "    return stemmed_tokens"
      ],
      "metadata": {
        "id": "fYJnwilJTh6n"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating the list of all words known to us so far in preparation for training\n",
        "# our sentiment analysis models\n",
        "vocab = []\n",
        "for comment in x_train[:, 0]:\n",
        "  for token in clean(comment, True):\n",
        "    if token not in vocab:\n",
        "      vocab.append(token)"
      ],
      "metadata": {
        "id": "WU7CG6jYgPCt"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing a function to filter out words which aren't in our vocabulary, as the\n",
        "# vocab vector that we will later use for model training will not contain\n",
        "# a slot associated with those words\n",
        "def filter(text):\n",
        "  text_new = []\n",
        "  for token in clean(text, True):\n",
        "    if token in vocab:\n",
        "      text_new.append(token)\n",
        "  return \" \".join(text_new)"
      ],
      "metadata": {
        "id": "98j8FiEvtpX7"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the train data even further to test the performance of our models\n",
        "# without getting any information about the simulated users\n",
        "Xtr, Xte, Ytr, Yte = train_test_split(x_train, y_train, test_size = 0.3, \n",
        "                                                    random_state = 0)\n",
        "tv = TfidfVectorizer(ngram_range = (1, 2), vocabulary = vocab)"
      ],
      "metadata": {
        "id": "pqvSWKTAo3h9"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Requesting sentiment data and a vectoriser to return machine-learning-ready \n",
        "# data; a pickle option is also there to preserve the vectoriser configuration\n",
        "# for future pre-processing â”€ this is necessary so that the dimensions of our\n",
        "# vectorised data are always the same\n",
        "def pre_process_1(x, vct, is_pickle):\n",
        "  x_comments = [clean(i) for i in x[:, 0]]\n",
        "  x_clean = np.array([[x_comments[i], x[i, 1]] for i in range(len(x))])\n",
        "  x_trim = [[filter(i[0]), i[1]] for i in x_clean]\n",
        "  x_comments = [i[0] for i in x_trim]\n",
        "\n",
        "  x_vector = vct.fit_transform(x_comments).toarray()\n",
        "  if is_pickle:\n",
        "    s = pickle.dumps(vct)\n",
        "    global tv2\n",
        "    tv2 = pickle.loads(s)\n",
        "  x_final = []\n",
        "  # Encoding social medium source data\n",
        "  onehot = pd.get_dummies([i[1] for i in x_trim])\n",
        "  for i in range(len(x_trim)):\n",
        "    x_final.append(np.concatenate([x_vector[i],\n",
        "                                    onehot.iloc[i, :].tolist()]).tolist())\n",
        "  return np.array(x_final)"
      ],
      "metadata": {
        "id": "jtDXFs5DVFeJ"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-processing the training data and defining a pre-processing function with the\n",
        "# previously configured vectoriser\n",
        "Xtr_final = pre_process_1(Xtr, tv, True)\n",
        "def pre_process_2(x):\n",
        "  return pre_process_1(x, tv2, False)"
      ],
      "metadata": {
        "id": "S8b8lu8_aRRV"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading a pre-templated sentiment analysis engine for comparison\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp.add_pipe('spacytextblob')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8sTOLe7lMi0",
        "outputId": "c43fc5cb-46ce-4ddf-8145-1695dc755ff3"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacytextblob.spacytextblob.SpacyTextBlob at 0x7fedca50f450>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the performance of the pre-templated engine\n",
        "count = 0\n",
        "pos_correct = 0\n",
        "irr_correct = 0\n",
        "neg_correct = 0\n",
        "\n",
        "for comment in [clean(i) for i in Xte[:, 0]]:\n",
        "  sent = nlp(comment)._.blob.polarity\n",
        "  if Yte[count] == 'P':\n",
        "    if sent >= 0.5:\n",
        "      pos_correct += 1\n",
        "  if Yte[count] == 'O':\n",
        "    if sent > -0.5 and sent < 0.5:\n",
        "      pos_correct += 1\n",
        "  else:\n",
        "    if sent <= -0.5:\n",
        "      neg_correct += 1\n",
        "  count += 1\n",
        "\n",
        "spacy_accuracy = sum([pos_correct, irr_correct, neg_correct])/count\n",
        "\n",
        "# Writing a function for getting predictions on input data from the engine\n",
        "def spacy_pred(x):\n",
        "  pred = []\n",
        "  for comment in [clean(i) for i in Xte[:, 0]]:\n",
        "    pred.append(nlp(comment)._.blob.polarity)\n",
        "  return np.array(pred)\n",
        "\n",
        "# Recording the performance of the engine\n",
        "scores = [[spacy_accuracy, spacy_pred]]"
      ],
      "metadata": {
        "id": "0kKMyvgbmTYL"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training our own model and recording the performance; Naive Bayes is known to be\n",
        "# an effective classifier of largely independent data such as a simple list of words\n",
        "model = MultinomialNB()\n",
        "model.fit(Xtr_final, Ytr)\n",
        "s = pickle.dumps(model)\n",
        "def Bayes(x):\n",
        "  return pickle.loads(s).predict(pre_process_2(x))\n",
        "\n",
        "scores.append([accuracy_score(Yte, Bayes(Xte)), Bayes])"
      ],
      "metadata": {
        "id": "qE0-b-ZVO-gg"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training our neural network\n",
        "model = MLPClassifier()\n",
        "model.fit(Xtr_final, Ytr)\n",
        "s = pickle.dumps(model)\n",
        "def MLP(x):\n",
        "  return pickle.loads(s).predict(pre_process_2(x))\n",
        "\n",
        "scores.append([accuracy_score(Yte, MLP(Xte)), MLP])"
      ],
      "metadata": {
        "id": "JHd3M_i0iftL"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Outputing the performance of each model\n",
        "scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dE4oRuoluBV",
        "outputId": "d1a49be0-8214-4f80-85a2-6b181d5e39e7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.6288156288156288, <function __main__.spacy_pred>],\n",
              " [0.7032967032967034, <function __main__.Bayes>],\n",
              " [0.684981684981685, <function __main__.MLP>]]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# While the custom-trained models performed better, they consumed all of Google\n",
        "# Colab's RAM and frequently crashed the sessions, forcing me to use spacy's\n",
        "# pre-trained model\n",
        "\n",
        "sent_an = spacy_pred"
      ],
      "metadata": {
        "id": "JOEMjUCZWe7C"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a function to compute the sentiment scores of a user\n",
        "def arasapps(user, aras = True):\n",
        "  if aras:\n",
        "    pubs = user.replies\n",
        "  else:\n",
        "    pubs = user.comments\n",
        "  x = [[i.content, i.source] for i in pubs]\n",
        "  return mean(sent_an(np.array(x)))"
      ],
      "metadata": {
        "id": "jOI4ysW6UTZq"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a function to compute the sentiment scores of a list of publications\n",
        "def arasapps_list(pubs):\n",
        "  return mean(sent_an(np.array(pubs)))"
      ],
      "metadata": {
        "id": "WuOk1DI4vpxn"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating a thread designed to simulate users engaging in social media activities:\n",
        "# writing comments, receiving replies, and joining, leaving, and requesting\n",
        "# access to, rooms\n",
        "def publication_sample():\n",
        "    return [simul.iloc[:,[0,2]].sample().iloc[0,i] for i in range(2)]\n",
        "\n",
        "class Event:\n",
        "    # Exactly one of the following is NOT 0 or None:\n",
        "    # - status_change_in_room = 0 - no change, 1 or 2 - room number\n",
        "    # - comment = [text, source] or None\n",
        "    # - reply = [text, source] or None\n",
        "    def __init__(self, user_index, status_change_in_room, comment, reply):\n",
        "        self.user_index = user_index\n",
        "        self.date = 5\n",
        "        self.timestamp = time.time_ns()\n",
        "        self.status_change_in_room = status_change_in_room\n",
        "        self.comment = comment\n",
        "        self.reply = reply\n",
        "\n",
        "    def __str__(self):\n",
        "        if self.status_change_in_room != 0:\n",
        "            return f\"User {self.user_index}: Status change in room {self.status_change_in_room} at {self.timestamp}\"\n",
        "        else:\n",
        "            what = 'Comment' if self.comment is not None else 'Reply'\n",
        "            data = self.comment if self.comment is not None else self.reply\n",
        "            return f\"User {self.user_index}: {what} '{data[0]}' from {data[1]} at {self.timestamp}\"\n",
        "\n",
        "\n",
        "def generate_event(user_index, total_rooms):\n",
        "    event_type = random.randrange(100)  # 40% - comment, next 40% - reply, next 20% - status change\n",
        "    if event_type < 40:     # comment\n",
        "        return Event(user_index, 0, publication_sample(), None)\n",
        "    elif event_type < 80:   # reply\n",
        "        return Event(user_index, 0, None, publication_sample())\n",
        "    else:\n",
        "        return Event(user_index, random.randint(1, total_rooms), None, None)\n",
        "\n",
        "\n",
        "def wait_then_generate_event(user_index, total_rooms, event_queue):\n",
        "    while True:\n",
        "        time.sleep(random.randint(10, 110))  # uniform distribution around 60 seconds\n",
        "        event_queue.put(generate_event(user_index, total_rooms))\n",
        "\n",
        "\n",
        "def start_event_generation_threads(total_users, rooms, event_queue):\n",
        "    threads = []\n",
        "    for user_index in range(0, total_users):\n",
        "        args = [user_index, len(rooms), event_queue]\n",
        "        x = threading.Thread(target=wait_then_generate_event, args=args)\n",
        "        threads.append(x)\n",
        "    for x in threads:\n",
        "        x.start()\n",
        "    return threads"
      ],
      "metadata": {
        "id": "r1oBv237zekB"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_scores = []\n",
        "for user in users:\n",
        "  sent_scores.append([arasapps(user), arasapps(user, False)])"
      ],
      "metadata": {
        "id": "ODCGmWBCo81C"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_users_sentiments_pie_chart(sentiment_list):\n",
        "  all_labels = ['<0.2', '[0.2,0.4)', '[0.4,0.6)', '[0.6,0.8)', '>0.8']\n",
        "  boundaries = [0.2, 0.4, 0.6, 0.8, 1]\n",
        "  classes = [0 for _ in range(len(all_labels))]\n",
        "  for s in sentiment_list:\n",
        "      for bound_index in range(len(boundaries)):\n",
        "          if s < boundaries[bound_index]:\n",
        "              classes[bound_index] += 1\n",
        "              break\n",
        "  # don't show the missing classes\n",
        "  labels = []\n",
        "  values = []\n",
        "  for i in range(len(classes)):\n",
        "      if classes[i] > 0:\n",
        "          labels.append(all_labels[i])\n",
        "          values.append(classes[i])\n",
        "  fig1, ax1 = plt.subplots()\n",
        "  plt.title('User/sentiments')\n",
        "  ax1.pie(values, labels=labels, autopct='%1.1f%%', shadow=False, startangle=90)\n",
        "  ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def show_comments_or_replies_bar_charts(comments, scores, title):\n",
        "    date_to_scores = {}\n",
        "    source_to_scores = {}\n",
        "    for i in range(len(comments)):\n",
        "        c = comments[i]\n",
        "        date_to_scores.setdefault(c.date, []).append(scores[i])\n",
        "        source_to_scores.setdefault(c.source, []).append(scores[i])\n",
        "    date_to_average = {}\n",
        "    for d in date_to_scores.keys():\n",
        "        date_to_average[d] = sum(date_to_scores[d]) / len(date_to_scores[d])\n",
        "    source_to_average = {}\n",
        "    for s in source_to_scores.keys():\n",
        "        source_to_average[s] = sum(source_to_scores[s]) / len(source_to_scores[s])\n",
        "    dates = sorted(date_to_average.keys())\n",
        "    date_scores = [date_to_average.get(d) for d in dates]\n",
        "    plt.title(title + \"date\")\n",
        "    show_bar_plot(dates, date_scores)\n",
        "    sources = sorted(source_to_average.keys())\n",
        "    source_scores = [source_to_average.get(s) for s in sources]\n",
        "    plt.title(title + \"source\")\n",
        "    show_bar_plot(sources, source_scores)\n",
        "\n",
        "\n",
        "def show_user_graphs(user):\n",
        "    comment_scores = [arasapps_list([c]) for c in user.comments]\n",
        "    reply_scores = [arasapps_list([r]) for r in user.replies]\n",
        "    if len(comment_scores):\n",
        "        show_comments_or_replies_bar_charts(user.comments, comment_scores, f\"{user.name}: APPS per \")\n",
        "    if len(reply_scores):\n",
        "        show_comments_or_replies_bar_charts(user.replies, reply_scores, f\"{user.name}: ARAS per \")\n",
        "\n",
        "\n",
        "def show_bar_plot(xlist, ylist, align='center'):\n",
        "    \"\"\"align = 'center' or 'edge' \"\"\"\n",
        "    plt.bar(xlist, ylist, width=0.7, align=align, linewidth=0, color='lightslategrey')\n",
        "    plt.show()\n",
        "def process_new_events(new_events):\n",
        "  user_num_pubs = [[len(i.comments), len(i.replies)] for i in users]\n",
        "  user_new_pubs = [[[], []] for i in range(len(users))]\n",
        "  for event in new_events:\n",
        "    user = users[event.user_index]\n",
        "    if event.comment is not None:\n",
        "      comment = event.comment\n",
        "      user_new_pubs[event.user_index][0].append(comment)\n",
        "      user.comments.append(Publication(comment[0], None, comment[1], event.date,\n",
        "                          event.timestamp))\n",
        "    elif event.reply is not None:\n",
        "      reply = event.reply\n",
        "      user.replies.append(Publication(reply[0], None, reply[1], event.date,\n",
        "                          event.timestamp))\n",
        "      user_new_pubs[event.user_index][1].append(reply)\n",
        "    else:\n",
        "      room_index = event.status_change_in_room - 1\n",
        "      if user in rooms[room_index].members:\n",
        "        rooms[room_index].members.remove(user)\n",
        "      elif user in rooms[room_index].join_requests:\n",
        "        rooms[room_index].join_requests.remove(user)\n",
        "      else:\n",
        "        rooms[room_index].join_requests.append(user)\n",
        "  for user in users:\n",
        "    com = arasapps_list(user_new_pubs[user.userID][0])\n",
        "    rep = arasapps_list(user_new_pubs[user.userID][1])\n",
        "    comold = sent_scores[user.userID][0]\n",
        "    repold = sent_scores[user.userID][1]\n",
        "    lencomold = user_num_pubs[user.userID][0]\n",
        "    lenrepold = user_num_pubs[user.userID][1]\n",
        "    if len(com) + lencomold > 0:\n",
        "      sent_scores[user.userID][0] = [(com*len(com)+comold*lencomold)/(len(com)+lencomold)]\n",
        "    if len(rep) + lenrepold > 0:\n",
        "      sent_scores[user.userID][1] = [(rep*len(rep)+repold*lenrepold)/(len(rep)+lenrepold)]\n",
        "  for room in rooms:\n",
        "    print(room.name)\n",
        "    print(\"\\t\", \"MEMBERS\")\n",
        "    for member in room.members:\n",
        "      status = \"safe\"\n",
        "      if sent_scores[member.userID][0] < -0.3:\n",
        "        status = \"likely troll\"\n",
        "      if sent_scores[member.userID][1] < -0.3:\n",
        "        status = \"likely bully\"\n",
        "      if sent_scores[member.userID][0] < -0.3 and sent_scores[member.userID][1] < -0.3:\n",
        "        status = \"high-threat\"\n",
        "      print(member.name, sent_scores[member.userID], status)\n",
        "    print(\"\\t\", \"JOIN REQUESTS\")\n",
        "    for member in room.join_requests:\n",
        "      status = \"safe\"\n",
        "      if sent_scores[member.userID][0] < -0.3:\n",
        "        status = \"likely troll\"\n",
        "      if sent_scores[member.userID][1] < -0.3:\n",
        "        status = \"likely bully\"\n",
        "      if sent_scores[member.userID][0] < -0.3 and sent_scores[member.userID][1] < -0.3:\n",
        "        status = \"high-threat\"\n",
        "      print(member.name, sent_scores[member.userID], status, \"accept/decline\")"
      ],
      "metadata": {
        "id": "IiYv2uixnd_B"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "event_queue = queue.SimpleQueue()\n",
        "start_event_generation_threads(len(users), rooms, event_queue)\n",
        "while True:\n",
        "    new_events = []\n",
        "    while not event_queue.empty():\n",
        "        new_events.append(event_queue.get())\n",
        "    if len(new_events):\n",
        "        process_new_events(new_events)\n",
        "        user = choice(users)\n",
        "        sentiments = sent_scores\n",
        "        show_users_sentiments_pie_chart(sentiments)\n",
        "        show_user_graphs(user)\n",
        "    time.sleep(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "EKnQvjEVd7fb",
        "outputId": "11ee3a41-63cc-4821-93e7-de7573fa9b0f"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-f5cc53d434f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mshow_users_sentiments_pie_chart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentiments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mshow_user_graphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}